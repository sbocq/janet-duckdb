(import ../src/db)
(use judge)

(test (with [db (db/open)]
        (db/library-version))
  "v1.3.1")

(test (with [db (db/open)]
        (db/config-flags-table))
  @{"access_mode" "Access mode of the database (AUTOMATIC, READ_ONLY or READ_WRITE)"
    "allocator_background_threads" "Whether to enable the allocator background thread."
    "allocator_bulk_deallocation_flush_threshold" "If a bulk deallocation larger than this occurs, flush outstanding allocations."
    "allocator_flush_threshold" "Peak allocation threshold at which to flush the allocator after completing a task."
    "allow_community_extensions" "Allow to load community built extensions"
    "allow_extensions_metadata_mismatch" "Allow to load extensions with not compatible metadata"
    "allow_persistent_secrets" "Allow the creation of persistent secrets, that are stored and loaded on restarts"
    "allow_unredacted_secrets" "Allow printing unredacted secrets"
    "allow_unsigned_extensions" "Allow to load extensions with invalid or missing signatures"
    "allowed_directories" "List of directories/prefixes that are ALWAYS allowed to be queried - even when enable_external_access is false"
    "allowed_paths" "List of files that are ALWAYS allowed to be queried - even when enable_external_access is false"
    "arrow_large_buffer_size" "Whether Arrow buffers for strings, blobs, uuids and bits should be exported using large buffers"
    "arrow_lossless_conversion" "Whenever a DuckDB type does not have a clear native or canonical extension match in Arrow, export the types with a duckdb.type_name extension name."
    "arrow_output_list_view" "Whether export to Arrow format should use ListView as the physical layout for LIST columns"
    "arrow_output_version" "Whether strings should be produced by DuckDB in Utf8View format instead of Utf8"
    "asof_loop_join_threshold" "The maximum number of rows we need on the left side of an ASOF join to use a nested loop join"
    "autoinstall_extension_repository" "Overrides the custom endpoint for extension installation on autoloading"
    "autoinstall_known_extensions" "Whether known extensions are allowed to be automatically installed when a query depends on them"
    "autoload_known_extensions" "Whether known extensions are allowed to be automatically loaded when a query depends on them"
    "azure_account_name" "azure"
    "azure_context_caching" "azure"
    "azure_credential_chain" "azure"
    "azure_endpoint" "azure"
    "azure_http_proxy" "azure"
    "azure_http_stats" "azure"
    "azure_proxy_password" "azure"
    "azure_proxy_user_name" "azure"
    "azure_read_buffer_size" "azure"
    "azure_read_transfer_chunk_size" "azure"
    "azure_read_transfer_concurrency" "azure"
    "azure_storage_connection_string" "azure"
    "azure_transport_option_type" "azure"
    "binary_as_string" "parquet"
    "ca_cert_file" "httpfs"
    "calendar" "icu"
    "catalog_error_max_schemas" "The maximum number of schemas the system will scan for \"did you mean...\" style errors in the catalog"
    "checkpoint_threshold" "The WAL size threshold at which to automatically trigger a checkpoint (e.g. 1GB)"
    "custom_extension_repository" "Overrides the custom endpoint for remote extension installation"
    "custom_profiling_settings" "Accepts a JSON enabling custom metrics"
    "custom_user_agent" "Metadata from DuckDB callers"
    "debug_asof_iejoin" "DEBUG SETTING: force use of IEJoin to implement AsOf joins"
    "debug_checkpoint_abort" "DEBUG SETTING: trigger an abort while checkpointing for testing purposes"
    "debug_force_external" "DEBUG SETTING: force out-of-core computation for operators that support it, used for testing"
    "debug_force_no_cross_product" "DEBUG SETTING: Force disable cross product generation when hyper graph isn't connected, used for testing"
    "debug_skip_checkpoint_on_commit" "DEBUG SETTING: skip checkpointing on commit"
    "debug_verify_vector" "DEBUG SETTING: enable vector verification"
    "debug_window_mode" "DEBUG SETTING: switch window mode to use"
    "default_block_size" "The default block size for new duckdb database files (new as-in, they do not yet exist)."
    "default_collation" "The collation setting used when none is specified"
    "default_null_order" "NULL ordering used when none is specified (NULLS_FIRST or NULLS_LAST)"
    "default_order" "The order type used when none is specified (ASC or DESC)"
    "default_secret_storage" "Allows switching the default storage for secrets"
    "disable_parquet_prefetching" "parquet"
    "disable_timestamptz_casts" "Disable casting from timestamp to timestamptz "
    "disabled_compression_methods" "Disable a specific set of compression methods (comma separated)"
    "disabled_filesystems" "Disable specific file systems preventing access (e.g. LocalFileSystem)"
    "disabled_log_types" "Sets the list of disabled loggers"
    "disabled_optimizers" "DEBUG SETTING: disable a specific set of optimizers (comma separated)"
    "duckdb_api" "DuckDB API surface"
    "dynamic_or_filter_threshold" "The maximum amount of OR filters we generate dynamically from a hash join"
    "enable_external_access" "Allow the database to access external state (through e.g. loading/installing modules, COPY TO/FROM, CSV readers, pandas replacement scans, etc)"
    "enable_external_file_cache" "Allow the database to cache external files (e.g., Parquet) in memory."
    "enable_fsst_vectors" "Allow scans on FSST compressed segments to emit compressed vectors to utilize late decompression"
    "enable_geoparquet_conversion" "parquet"
    "enable_http_logging" "Enables HTTP logging"
    "enable_http_metadata_cache" "Whether or not the global http metadata is used to cache HTTP metadata"
    "enable_logging" "Enables the logger"
    "enable_macro_dependencies" "Enable created MACROs to create dependencies on the referenced objects (such as tables)"
    "enable_object_cache" "[PLACEHOLDER] Legacy setting - does nothing"
    "enable_profiling" "Enables profiling, and sets the output format (JSON, QUERY_TREE, QUERY_TREE_OPTIMIZER)"
    "enable_progress_bar" "Enables the progress bar, printing progress to the terminal for long queries"
    "enable_progress_bar_print" "Controls the printing of the progress bar, when 'enable_progress_bar' is true"
    "enable_server_cert_verification" "httpfs"
    "enable_view_dependencies" "Enable created VIEWs to create dependencies on the referenced objects (such as tables)"
    "enabled_log_types" "Sets the list of enabled loggers"
    "errors_as_json" "Output error messages as structured JSON instead of as a raw string"
    "explain_output" "Output of EXPLAIN statements (ALL, OPTIMIZED_ONLY, PHYSICAL_ONLY)"
    "extension_directory" "Set the directory to store extensions in"
    "external_threads" "The number of external threads that work on DuckDB tasks."
    "file_search_path" "A comma separated list of directories to search for input files"
    "force_bitpacking_mode" "DEBUG SETTING: forces a specific bitpacking mode"
    "force_compression" "DEBUG SETTING: forces a specific compression method to be used"
    "force_download" "httpfs"
    "hf_max_per_page" "httpfs"
    "hnsw_ef_search" "vss"
    "hnsw_enable_experimental_persistence" "vss"
    "home_directory" "Sets the home directory used by the system"
    "http_keep_alive" "httpfs"
    "http_logging_output" "The file to which HTTP logging output should be saved, or empty to print to the terminal"
    "http_proxy" "HTTP proxy host"
    "http_proxy_password" "Password for HTTP proxy"
    "http_proxy_username" "Username for HTTP proxy"
    "http_retries" "httpfs"
    "http_retry_backoff" "httpfs"
    "http_retry_wait_ms" "httpfs"
    "http_timeout" "httpfs"
    "ieee_floating_point_ops" "Use IEE754-compliant floating point operations (returning NAN instead of errors/NULL)."
    "immediate_transaction_mode" "Whether transactions should be started lazily when needed, or immediately when BEGIN TRANSACTION is called"
    "index_scan_max_count" "The maximum index scan count sets a threshold for index scans. If fewer than MAX(index_scan_max_count, index_scan_percentage * total_row_count) rows match, we perform an index scan instead of a table scan."
    "index_scan_percentage" "The index scan percentage sets a threshold for index scans. If fewer than MAX(index_scan_max_count, index_scan_percentage * total_row_count) rows match, we perform an index scan instead of a table scan."
    "integer_division" "Whether or not the / operator defaults to integer division, or to floating point division"
    "lambda_syntax" "Configures the use of the deprecated single arrow operator (->) for lambda functions."
    "late_materialization_max_rows" "The maximum amount of rows in the LIMIT/SAMPLE for which we trigger late materialization"
    "lock_configuration" "Whether or not the configuration can be altered"
    "log_query_path" "Specifies the path to which queries should be logged (default: NULL, queries are not logged)"
    "logging_level" "The log level which will be recorded in the log"
    "logging_mode" "Enables the logger"
    "logging_storage" "Set the logging storage (memory/stdout/file)"
    "max_expression_depth" "The maximum expression depth limit in the parser. WARNING: increasing this setting and using very deep expressions might lead to stack overflow errors."
    "max_memory" "The maximum memory of the system (e.g. 1GB)"
    "max_temp_directory_size" "The maximum amount of data stored inside the 'temp_directory' (when set) (e.g. 1GB)"
    "max_vacuum_tasks" "The maximum vacuum tasks to schedule during a checkpoint."
    "memory_limit" "The maximum memory of the system (e.g. 1GB)"
    "merge_join_threshold" "The number of rows we need on either table to choose a merge join"
    "mysql_bit1_as_boolean" "mysql_scanner"
    "mysql_debug_show_queries" "mysql_scanner"
    "mysql_experimental_filter_pushdown" "mysql_scanner"
    "mysql_tinyint1_as_boolean" "mysql_scanner"
    "nested_loop_join_threshold" "The number of rows we need on either table to choose a nested loop join"
    "null_order" "NULL ordering used when none is specified (NULLS_FIRST or NULLS_LAST)"
    "old_implicit_casting" "Allow implicit casting to/from VARCHAR"
    "order_by_non_integer_literal" "Allow ordering by non-integer literals - ordering by such literals has no effect."
    "ordered_aggregate_threshold" "The number of rows to accumulate before sorting, used for tuning"
    "parquet_metadata_cache" "parquet"
    "partitioned_write_flush_threshold" "The threshold in number of rows after which we flush a thread state when writing using PARTITION_BY"
    "partitioned_write_max_open_files" "The maximum amount of files the system can keep open before flushing to disk when writing using PARTITION_BY"
    "password" "The password to use. Ignored for legacy compatibility."
    "perfect_ht_threshold" "Threshold in bytes for when to use a perfect hash table"
    "pg_array_as_varchar" "postgres_scanner"
    "pg_connection_cache" "postgres_scanner"
    "pg_connection_limit" "postgres_scanner"
    "pg_debug_show_queries" "postgres_scanner"
    "pg_experimental_filter_pushdown" "postgres_scanner"
    "pg_null_byte_replacement" "postgres_scanner"
    "pg_pages_per_task" "postgres_scanner"
    "pg_use_binary_copy" "postgres_scanner"
    "pg_use_ctid_scan" "postgres_scanner"
    "pivot_filter_threshold" "The threshold to switch from using filtered aggregates to LIST with a dedicated pivot operator"
    "pivot_limit" "The maximum number of pivot columns in a pivot statement"
    "prefer_range_joins" "Force use of range joins with mixed predicates"
    "prefetch_all_parquet_files" "parquet"
    "preserve_identifier_case" "Whether or not to preserve the identifier case, instead of always lowercasing all non-quoted identifiers"
    "preserve_insertion_order" "Whether or not to preserve insertion order. If set to false the system is allowed to re-order any results that do not contain ORDER BY clauses."
    "produce_arrow_string_view" "Whether strings should be produced by DuckDB in Utf8View format instead of Utf8"
    "profile_output" "The file to which profile output should be saved, or empty to print to the terminal"
    "profiling_mode" "The profiling mode (STANDARD or DETAILED)"
    "profiling_output" "The file to which profile output should be saved, or empty to print to the terminal"
    "progress_bar_time" "Sets the time (in milliseconds) how long a query needs to take before we start printing a progress bar"
    "s3_access_key_id" "httpfs"
    "s3_endpoint" "httpfs"
    "s3_kms_key_id" "httpfs"
    "s3_region" "httpfs"
    "s3_secret_access_key" "httpfs"
    "s3_session_token" "httpfs"
    "s3_uploader_max_filesize" "httpfs"
    "s3_uploader_max_parts_per_file" "httpfs"
    "s3_uploader_thread_limit" "httpfs"
    "s3_url_compatibility_mode" "httpfs"
    "s3_url_style" "httpfs"
    "s3_use_ssl" "httpfs"
    "scalar_subquery_error_on_multiple_rows" "When a scalar subquery returns multiple rows - return a random row instead of returning an error."
    "scheduler_process_partial" "Partially process tasks before rescheduling - allows for more scheduler fairness between separate queries"
    "schema" "Sets the default search schema. Equivalent to setting search_path to a single value."
    "search_path" "Sets the default catalog search path as a comma-separated list of values"
    "secret_directory" "Set the directory to which persistent secrets are stored"
    "sqlite_all_varchar" "sqlite_scanner"
    "sqlite_debug_show_queries" "sqlite_scanner"
    "storage_compatibility_version" "Serialize on checkpoint with compatibility for a given duckdb version"
    "streaming_buffer_size" "The maximum memory to buffer between fetching from a streaming result (e.g. 1GB)"
    "temp_directory" "Set the directory to which to write temp files"
    "threads" "The number of total threads used by the system."
    "timezone" "icu"
    "ui_local_port" "ui"
    "ui_polling_interval" "ui"
    "ui_remote_url" "ui"
    "unsafe_enable_version_guessing" "iceberg"
    "user" "The username to use. Ignored for legacy compatibility."
    "username" "The username to use. Ignored for legacy compatibility."
    "wal_autocheckpoint" "The WAL size threshold at which to automatically trigger a checkpoint (e.g. 1GB)"
    "worker_threads" "The number of total threads used by the system."
    "zstd_min_string_length" "The (average) length at which to enable ZSTD compression, defaults to 4096"})

(test (with [db (db/open ":memory:" {:access_mode :automatic})]) nil)
